# configuration for cifar10 network.

[cifar10_stack]
input_def=f:4
label_def=i:1
layers=cost,f0,stack,pool,reshape,relu,relu2,softmax_in,out
active=out
cost=cost

learner=adam
learning_rate_type=exponential_decay
learning_rate=1e-3
learning_decay_ratio=0.96
learning_decay_step=1000

epoch=80
batch_size=128

f0.type=conv2d_pool
f0.input=input[0]
f0.shape=5,5,3,192
f0.pool_type=avg
f0.pool_size=3
f0.pool_strides=2

stack.type=stack_conv2d
stack.input=f0
stack.use_residual=1
stack.shape=5,5,192,64
stack.stack_count=10

pool.type=pooling
pool.input=stack
pool.pool_type=max
pool.pool_size=2
pool.pool_strides=2

reshape.type=reshape
reshape.input=pool
reshape.shape=-1,2304

relu.type=full_connect_op
relu.input=reshape
relu.op=relu
relu.l2wd=0.004
relu.n_in=2304
relu.n_out=256
relu.weight_stddev=0.04
relu.bias_init=0.1

relu2.type=full_connect_op
relu2.input=relu
relu2.op=relu
relu2.l2wd=0.004
relu2.n_in=256
relu2.n_out=192
relu2.weight_stddev=0.04
relu2.bias_init=0.1

softmax_in.type=full_connect
softmax_in.input=relu2
softmax_in.n_in=192
softmax_in.n_out=10
softmax_in.l2wd=0.00
softmax_in.weight_stddev=0.0052

out.type=softmax
out.input=softmax_in

cost.type=softmax_entropy
cost.input=softmax_in,__label__

[cifar10_simple_layer3]
input_def=f:4
label_def=i:1
layers=cost,cp1,cp2,cp3,reshape,relu,relu2,softmax_in,out
active=out
cost=cost

learner=adam
learning_rate_type=exponential_decay
learning_rate=1e-3
learning_decay_ratio=0.96
learning_decay_step=1000

epoch=80
batch_size=128

cp1.type=conv2d_pool
cp1.input=input[0]
cp1.shape=5,5,3,192
cp1.pool_type=avg
cp1.pool_size=3
cp1.pool_strides=2

cp2.type=conv2d_pool
cp2.input=cp1
cp2.shape=5,5,192,64
cp2.pool_type=max
cp2.pool_size=3
cp2.pool_strides=2

cp3.type=conv2d_pool
cp3.input=cp2
cp3.shape=5,5,64,64
cp3.pool_type=max
cp3.pool_size=3
cp3.pool_strides=2

reshape.type=reshape
reshape.input=cp3
reshape.shape=-1,576

relu.type=full_connect_op
relu.input=reshape
relu.op=relu
relu.l2wd=0.004
relu.n_in=576
relu.n_out=256
relu.weight_stddev=0.04
relu.bias_init=0.1

relu2.type=full_connect_op
relu2.input=relu
relu2.op=relu
relu2.l2wd=0.004
relu2.n_in=256
relu2.n_out=192
relu2.weight_stddev=0.04
relu2.bias_init=0.1

softmax_in.type=full_connect
softmax_in.input=relu2
softmax_in.n_in=192
softmax_in.n_out=10
softmax_in.l2wd=0.00
softmax_in.weight_stddev=0.0052

out.type=softmax
out.input=softmax_in

cost.type=softmax_entropy
cost.input=softmax_in,__label__


# this net is copied from tensorflow tutorial.
[cifar10_tf_tutor]
input_def=f:4
label_def=i:1
layers=cost,convpool1,norm1,conv2,norm2,pool2,reshape2,relu,relu2,softmax_in,out
active=out
cost=cost

learner=movingGradient
learning_rate_type=exponential_decay
learning_rate=1e-1
learning_decay_ratio=0.1
learning_decay_step=130000

epoch=80
batch_size=128

convpool1.type=conv2d_pool
convpool1.input=input[0]
convpool1.shape=5,5,3,64
convpool1.pool_type=max
convpool1.pool_size=3
convpool1.pool_strides=2

norm1.type=local_norm
norm1.input=convpool1

conv2.type=conv2d
conv2.input=norm1
conv2.shape=3,3,64,64

norm2.type=local_norm
norm2.input=conv2

pool2.type=pooling
pool2.input=norm2
pool2.pool_type=max
pool2.pool_size=3
pool2.pool_strides=2

reshape2.type=reshape
reshape2.input=pool2
# 2304@24 4096@32
reshape2.shape=-1,2304

relu.type=full_connect_op
relu.input=reshape2
relu.op=relu
relu.l2wd=0.004
# 2304@24 4096@32
relu.n_in=2304
relu.n_out=384
relu.weight_stddev=0.04
relu.bias_init=0.1

relu2.type=full_connect_op
relu2.input=relu
relu2.op=relu
relu2.l2wd=0.004
relu2.n_in=384
relu2.n_out=192
relu2.weight_stddev=0.04
relu2.bias_init=0.1

softmax_in.type=full_connect
softmax_in.input=relu2
softmax_in.n_in=192
softmax_in.n_out=10
softmax_in.l2wd=0.00
softmax_in.weight_stddev=0.0052

out.type=softmax
out.input=softmax_in

cost.type=softmax_entropy
cost.input=softmax_in,__label__

