# configuration for cifar10 network.

# test maxout/dropout/residual on cnn3+fc
# fetch 87% ! @53600 iteration.(137 epoch)
[cifar10_cnn3_fc]
gpu_portion=0.3
input_def=f:4
label_def=i:1
layers=c0,c1,c2,reshape,relu,softmax_in,out,cost
active=out
cost=cost

learner=adam
learning_rate_type=exponential_decay
learning_rate=1e-3
learning_decay_ratio=0.98
learning_decay_step=1000

epoch=500
batch_size=128

c0.type=conv2d_pool
c0.input=input[0]
c0.shape=3,3,3,192
c0.pool_type=avg
c0.pool_size=2
c0.pool_strides=2

c1.type=conv2d_pool
c1.input=c0
c1.shape=3,3,192,256
c1.pool_type=avg
c1.pool_size=2
c1.pool_strides=2

c2.type=conv2d_pool
c2.input=c1
c2.shape=3,3,256,256
c2.pool_type=avg
c2.pool_size=2
c2.pool_strides=2

reshape.type=reshape
reshape.input=c2
reshape.shape=-1,2304

relu.type=full_connect_op
relu.input=reshape
relu.op=relu
relu.l2wd=0.004
relu.n_in=2304
relu.n_out=1024
relu.weight_stddev=0.04
relu.bias_init=0.05

softmax_in.type=full_connect
softmax_in.input=relu
softmax_in.n_in=1024
softmax_in.n_out=10
softmax_in.l2wd=0.00
softmax_in.weight_stddev=0.0052

out.type=softmax
out.input=softmax_in

cost.type=softmax_entropy
cost.input=softmax_in,__label__



[cifar10_test_3x3_stack]
input_def=f:4
label_def=i:1
layers=cost,stack0,dropout,stack1,reshape,relu,relu2,softmax_in,out
active=out
cost=cost

learner=adam
learning_rate_type=exponential_decay
learning_rate=1e-3
learning_decay_ratio=0.98
learning_decay_step=1000

epoch=500
batch_size=32

stack0.type=stack_conv2d_pool
stack0.input=input[0]
stack0.stack_count=3
stack0.shape=3,3,3,64
stack0.use_residual=0

dropout.type=dropout
dropout.input=stack0
dropout.prob=0.6

stack1.type=stack_conv2d_pool
stack1.input=dropout
stack1.stack_count=5
stack1.shape=3,3,64,64
stack1.pool_type=avg
stack1.pool_size=2
stack1.pool_strides=2

reshape.type=reshape
reshape.input=stack1
reshape.shape=-1,9216

relu.type=full_connect_op
relu.input=reshape
relu.op=relu
relu.l2wd=0.004
relu.n_in=9216
relu.n_out=1024
relu.weight_stddev=0.04
relu.bias_init=0.1

relu2.type=full_connect_op
relu2.input=relu
relu2.op=relu
relu2.l2wd=0.004
relu2.n_in=1024
relu2.n_out=128
relu2.weight_stddev=0.04
relu2.bias_init=0.1

softmax_in.type=full_connect
softmax_in.input=relu2
softmax_in.n_in=128
softmax_in.n_out=10
softmax_in.l2wd=0.00
softmax_in.weight_stddev=0.0052

out.type=softmax
out.input=softmax_in

cost.type=softmax_entropy
cost.input=softmax_in,__label__


[cifar10_vgg_small]
input_def=f:4
label_def=i:1
layers=cost,f0,stack0,D1,stack1,stack2,D2,reshape,relu,relu2,softmax_in,out
active=out
cost=cost

learner=adam
learning_rate_type=exponential_decay
learning_rate=1e-3
learning_decay_ratio=0.99
learning_decay_step=6000

epoch=500
batch_size=32

f0.type=stack_conv2d_pool
f0.input=input[0]
f0.stack_count=2
f0.shape=3,3,3,64
f0.use_residual=1
f0.pool_type=max
f0.pool_size=2
f0.pool_strides=2

stack0.type=stack_conv2d_pool
stack0.input=f0
stack0.stack_count=2
stack0.use_residual=1
stack0.shape=3,3,64,128
stack0.pool_type=max
stack0.pool_size=2
stack0.pool_strides=2

D1.type=dropout
D1.input=stack0
D1.prob=0.4

stack1.type=stack_conv2d_pool
stack1.input=D1
stack1.stack_count=4
stack1.shape=3,3,128,256
stack1.use_residual=1
stack1.pool_type=max
stack1.pool_size=2
stack1.pool_strides=2

stack2.type=stack_conv2d_pool
stack2.input=stack1
stack2.stack_count=4
stack2.shape=3,3,256,512
stack2.use_residual=1

D2.type=dropout
D2.input=stack2
D2.prob=0.4

reshape.type=reshape
reshape.input=D2
reshape.shape=-1,4608

relu.type=full_connect_op
relu.input=reshape
relu.op=relu
relu.l2wd=0.000
relu.n_in=4608
relu.n_out=1024
relu.weight_stddev=0.04
relu.bias_init=0.0

relu2.type=full_connect_op
relu2.input=relu
relu2.op=relu
relu2.l2wd=0.000
relu2.n_in=1024
relu2.n_out=128
relu2.weight_stddev=0.04
relu2.bias_init=0.0

softmax_in.type=full_connect
softmax_in.input=relu2
softmax_in.n_in=128
softmax_in.n_out=10
softmax_in.l2wd=0.00
softmax_in.weight_stddev=0.0052

out.type=softmax
out.input=softmax_in

cost.type=softmax_entropy
cost.input=softmax_in,__label__

[cifar10_stack]
input_def=f:4
label_def=i:1
layers=cost,f0,stack0,stack1,stack2,stack3,reshape,relu,relu2,softmax_in,out
active=out
cost=cost

learner=adam
learning_rate_type=exponential_decay
learning_rate=1e-3
learning_decay_ratio=0.99
learning_decay_step=1000

epoch=500
batch_size=128

f0.type=conv2d_pool
f0.input=input[0]
f0.shape=3,3,3,192
f0.pool_type=avg
f0.pool_size=3
f0.pool_strides=2

stack0.type=stack_conv2d_pool
stack0.input=f0
stack0.stack_count=4
stack0.use_residual=1
stack0.shape=3,3,192,64
stack0.pool_type=max
stack0.pool_size=2
stack0.pool_strides=2

stack1.type=stack_conv2d_pool
stack1.input=stack0
stack1.stack_count=4
stack1.use_residual=1
stack1.shape=5,5,64,32
stack1.pool_type=max
stack1.pool_size=2
stack1.pool_strides=2

stack2.type=stack_conv2d_pool
stack2.input=stack1
stack2.stack_count=3
stack2.use_residual=1
stack2.shape=3,3,32,32
stack2.pool_type=max
stack2.pool_size=2
stack2.pool_strides=1

stack3.type=stack_conv2d_pool
stack3.input=stack2
stack3.stack_count=4
stack3.use_residual=1
stack3.shape=5,5,32,32
stack3.pool_type=max
stack3.pool_size=2
stack3.pool_strides=1

reshape.type=reshape
reshape.input=stack3
reshape.shape=-1,288

relu.type=full_connect_op
relu.input=reshape
relu.op=relu
relu.l2wd=0.004
relu.n_in=288
relu.n_out=1024
relu.weight_stddev=0.04
relu.bias_init=0.1

relu2.type=full_connect_op
relu2.input=relu
relu2.op=relu
relu2.l2wd=0.004
relu2.n_in=1024
relu2.n_out=128
relu2.weight_stddev=0.04
relu2.bias_init=0.1

softmax_in.type=full_connect
softmax_in.input=relu2
softmax_in.n_in=128
softmax_in.n_out=10
softmax_in.l2wd=0.00
softmax_in.weight_stddev=0.0052

out.type=softmax
out.input=softmax_in

cost.type=softmax_entropy
cost.input=softmax_in,__label__

[cifar10_simple_layer3]
input_def=f:4
label_def=i:1
layers=cost,cp1,cp2,cp3,reshape,relu,relu2,softmax_in,out
active=out
cost=cost

learner=adam
learning_rate_type=exponential_decay
learning_rate=1e-3
learning_decay_ratio=0.96
learning_decay_step=1000

epoch=500
batch_size=128

cp1.type=conv2d_pool
cp1.input=input[0]
cp1.shape=5,5,3,192
cp1.pool_type=avg
cp1.pool_size=3
cp1.pool_strides=2

cp2.type=conv2d_pool
cp2.input=cp1
cp2.shape=5,5,192,64
cp2.pool_type=max
cp2.pool_size=3
cp2.pool_strides=2

cp3.type=conv2d_pool
cp3.input=cp2
cp3.shape=5,5,64,64
cp3.pool_type=max
cp3.pool_size=3
cp3.pool_strides=2

reshape.type=reshape
reshape.input=cp3
reshape.shape=-1,576

relu.type=full_connect_op
relu.input=reshape
relu.op=relu
relu.l2wd=0.004
relu.n_in=576
relu.n_out=256
relu.weight_stddev=0.04
relu.bias_init=0.1

relu2.type=full_connect_op
relu2.input=relu
relu2.op=relu
relu2.l2wd=0.004
relu2.n_in=256
relu2.n_out=192
relu2.weight_stddev=0.04
relu2.bias_init=0.1

softmax_in.type=full_connect
softmax_in.input=relu2
softmax_in.n_in=192
softmax_in.n_out=10
softmax_in.l2wd=0.00
softmax_in.weight_stddev=0.0052

out.type=softmax
out.input=softmax_in

cost.type=softmax_entropy
cost.input=softmax_in,__label__


# this net is copied from tensorflow tutorial.
# This network can fetch ~ 85% 86% precision.
[cifar10_tf_tutor]
input_def=f:4
label_def=i:1
layers=cost,convpool1,norm1,conv2,norm2,pool2,reshape2,relu,relu2,softmax_in,out
active=out
cost=cost

learner=movingGradient
learning_rate_type=exponential_decay
learning_rate=1e-1
learning_decay_ratio=0.1
learning_decay_step=130000

epoch=500
batch_size=128

convpool1.type=conv2d_pool
convpool1.input=input[0]
convpool1.shape=5,5,3,64
convpool1.pool_type=max
convpool1.pool_size=3
convpool1.pool_strides=2

norm1.type=local_norm
norm1.input=convpool1

conv2.type=conv2d
conv2.input=norm1
conv2.shape=3,3,64,64

norm2.type=local_norm
norm2.input=conv2

pool2.type=pooling
pool2.input=norm2
pool2.pool_type=max
pool2.pool_size=3
pool2.pool_strides=2

reshape2.type=reshape
reshape2.input=pool2
# 2304@24 4096@32
reshape2.shape=-1,2304

relu.type=full_connect_op
relu.input=reshape2
relu.op=relu
relu.l2wd=0.004
# 2304@24 4096@32
relu.n_in=2304
relu.n_out=384
relu.weight_stddev=0.04
relu.bias_init=0.1

relu2.type=full_connect_op
relu2.input=relu
relu2.op=relu
relu2.l2wd=0.004
relu2.n_in=384
relu2.n_out=192
relu2.weight_stddev=0.04
relu2.bias_init=0.1

softmax_in.type=full_connect
softmax_in.input=relu2
softmax_in.n_in=192
softmax_in.n_out=10
softmax_in.l2wd=0.00
softmax_in.weight_stddev=0.0052

out.type=softmax
out.input=softmax_in

cost.type=softmax_entropy
cost.input=softmax_in,__label__

